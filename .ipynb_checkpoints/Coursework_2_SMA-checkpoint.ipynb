{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoEncWHm5s28"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1V7EzFr6N7p"
   },
   "source": [
    "## Library installation/import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1B0__6AYci7"
   },
   "source": [
    "Install and import libraries that are used in multiple sections of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6K2gnqqfRsoS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s7ivGrSR5NaA"
   },
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install tweepy\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_S1J4oc6laA"
   },
   "source": [
    "## Configuring Twitter API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_k-DQXnYXyIO"
   },
   "source": [
    "Please note that the API keys below are the course leader's own API keys. You are allowed to use it to do some small tests, but please be careful because all students in the class now have a copy of it, and hence the limits can be easily exceeded.\n",
    "\n",
    "If your group has decided to use Twitter data, you can [apply for your own keys](https://developer.twitter.com/en/apply-for-access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "X0v0dZ8fCnqI"
   },
   "outputs": [],
   "source": [
    "#API Key: 4XVopc1gLxY3MaqJMMnFUmE2y\n",
    "#API Key Secret: NwcxaASpyB2vC5cwl2Xh3gkg3qpd5LfsutIJu29M8nTtkJdL1b\n",
    "#Bearer Token: AAAAAAAAAAAAAAAAAAAAAGXqaAEAAAAAnzaQhJwkS8rKcDIKkIgg3W7xr2k%3Dx23TzRFSGaH0IDkXQ33NGpu4yYwNUDg3q7uSBgg4cAB1Cp4fcJ\n",
    "\n",
    "\n",
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler('4XVopc1gLxY3MaqJMMnFUmE2y', 'NwcxaASpyB2vC5cwl2Xh3gkg3qpd5LfsutIJu29M8nTtkJdL1b')\n",
    "# auth.set_access_token('4XVopc1gLxY3MaqJMMnFUmE2y', 'NwcxaASpyB2vC5cwl2Xh3gkg3qpd5LfsutIJu29M8nTtkJdL1b')\n",
    "auth.request_token = 'AAAAAAAAAAAAAAAAAAAAAGXqaAEAAAAAnzaQhJwkS8rKcDIKkIgg3W7xr2k%3Dx23TzRFSGaH0IDkXQ33NGpu4yYwNUDg3q7uSBgg4cAB1Cp4fcJ'\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sBB2guxczux"
   },
   "source": [
    "## Downloading of new data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5EwgFk86EfS"
   },
   "source": [
    "**IMPORTANT NOTE:** Please do not run the cell below unless intending to download a new data set.\n",
    "\n",
    "Make sure that you change the parameters.\n",
    "\n",
    "Also, check the [Tweepy API reference](https://docs.tweepy.org/en/latest/api.html) to find out about other ways through which you can retrieve tweets, e.g., by specifying usernames.\n",
    "\n",
    "<font color=\"red\">**WARNING:** It is likely that the retrieved tweets, which will be displayed in succeeding cells, contain strong language that some might find offensive or disturbing.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dSQXfQVGCpmj"
   },
   "outputs": [],
   "source": [
    "#['created_at', 'id', 'id_str', 'full_text', 'truncated', 'display_text_range', 'entities', \n",
    "#'extended_entities', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', \n",
    "#'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', \n",
    "#'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', \n",
    "#'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'lang']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collect tweets\n",
    "query = \"#Marvel\" + \" -filter:retweets\"\n",
    "cutoff_date = \"2022-01-01\"\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=query, lang=\"en\", since_id=cutoff_date).items(3000)\n",
    "\n",
    "tweets_list = [[tweet.created_at, tweet.user.screen_name, tweet.user.location, tweet.text] for tweet in tweets]\n",
    "\n",
    "tweets_df = pd.DataFrame(data=tweets_list, columns=['date', 'user', 'location', 'text'])\n",
    "\n",
    "# A good idea to save downloaded tweets as CSV\n",
    "tweets_df.to_csv ('current_set.csv', quotechar='\"', encoding='utf8', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qumfu0NwYKqR"
   },
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giyVyGPAYUTK"
   },
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K23I1qBNZ_HQ"
   },
   "source": [
    "Below we provide some code for text cleaning. However, we encourage you to think of other ways to clean your data, e.g., by removing hashtags, removing usernames, removing duplicate tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XWzLqibrM0pa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: text_processed, dtype: object)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Uncomment the line below if loading from previously saved CSV\n",
    "tweets_df = pd.read_csv('current_set.csv', quotechar='\"', encoding='utf8')\n",
    "\n",
    "\n",
    "# Remove punctuation\n",
    "tweets_df['text_processed'] = tweets_df['text'].map(lambda x: re.sub('[,\\\\.!?]', ' ', x))\n",
    "\n",
    "# Remove unnecessary line breaks\n",
    "tweets_df['text_processed'] = tweets_df['text_processed'].map(lambda x: re.sub(r\"\\n\", '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "tweets_df['text_processed'] = tweets_df['text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Think of how else your data can be cleaned\n",
    "\n",
    "# Print out the first rows \n",
    "print(tweets_df['text_processed'].head())\n",
    "\n",
    "# Removing duplicate tweets?\n",
    "unique_tweets = list(set(list(tweets_df['text_processed'].values)))\n",
    "unique_tweets = [t for t in unique_tweets if t]\n",
    "\n",
    "print(unique_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22qj0jq3YklZ"
   },
   "source": [
    "## Exploration using a word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IIh30BjaWji"
   },
   "source": [
    "Generating a word cloud is one way by which you can check whether your data needs any further cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KqxuMbjk8EkA"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lw/b_cs_rw914qfv54w6sq8rrsm0000gn/T/ipykernel_2470/2940541879.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# generate a word cloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mwordcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_long_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# visualize the word cloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tm/lib/python3.7/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \"\"\"\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tm/lib/python3.7/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \"\"\"\n\u001b[1;32m    613\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tm/lib/python3.7/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[0;32m--> 404\u001b[0;31m                              \"got %d.\" % len(frequencies))\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# join the words of the different tweets together into one string\n",
    "long_string = ' '.join(unique_tweets)\n",
    "new_long_string = ' '.join(set(long_string.split(\" \")))\n",
    "\n",
    "# create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# generate a word cloud\n",
    "wordcloud.generate(new_long_string)\n",
    "\n",
    "# visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQyNo3FYYuED"
   },
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwAZb_l-7C-L"
   },
   "outputs": [],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tfx5EMNw_pIw"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Do you want to modify this by adding more stop words?\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "  return [[word for word in simple_preprocess(str(doc)) \n",
    "    if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "data = tweets_df.text_processed.values.tolist()\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "# create a dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# create a corpus\n",
    "texts = data_words\n",
    "\n",
    "# convert the corpus into a BoW representation\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOb8cNW3PVfk"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# set number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# build an LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "\n",
    "# print keywords in each topic\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RPtA2Yg7Hnm"
   },
   "outputs": [],
   "source": [
    "# visualise the topics\n",
    "!pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWueCQawPlXn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./'+str(num_topics))\n",
    "\n",
    "LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "  pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "  LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7UoGTRZY1pg"
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3RLsS3Ac8v7"
   },
   "source": [
    "This implementation is based on the lexicon- and rule-based [VADER](https://github.com/cjhutto/vaderSentiment) sentiment analysis tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gas4oUUIY8iF"
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hK8WbOHbA78"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "for tweet_text in unique_tweets:\n",
    "    vs = analyzer.polarity_scores(tweet_text)\n",
    "    print(tweet_text + '\\t' + str(vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT11HUQBY47n"
   },
   "source": [
    "# Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcanVP2udd24"
   },
   "source": [
    "This implementation is based on [spaCy's model](https://spacy.io/models/en#en_core_web_trf) using contextualised embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUWgWiLZYaBS"
   },
   "outputs": [],
   "source": [
    "!pip install spacy-transformers\n",
    "!python -m spacy download en_core_web_trf\n",
    "import spacy\n",
    "import en_core_web_trf\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAvsa90Gpwna"
   },
   "outputs": [],
   "source": [
    "for tweet_text in unique_tweets:\n",
    "  doc = nlp(tweet_text)\n",
    "  print(tweet_text)\n",
    "  for ne in doc.ents:\n",
    "    print('\\tNE found: ', ne.start_char, ne.end_char, ne.label_, tweet_text[ne.start_char:ne.end_char])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4Ub0m8zoZfH"
   },
   "source": [
    "# Named Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqLYLoM7ecLP"
   },
   "source": [
    "This implementation is based on [spaCy Entity Linker](https://github.com/egerber/spacy-entity-linker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvKRih-dYEDL"
   },
   "outputs": [],
   "source": [
    "!pip install spacy-entity-linker\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "import en_core_web_md\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBelwSqgeLjJ"
   },
   "outputs": [],
   "source": [
    "from spacy_entity_linker import EntityLinker\n",
    "!python -m spacy_entity_linker \"download_knowledge_base\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "#add to pipeline\n",
    "nlp.add_pipe('entityLinker', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnZZ0t32oCgZ"
   },
   "outputs": [],
   "source": [
    "for tweet_text in unique_tweets:\n",
    "  try:\n",
    "    doc = nlp(tweet_text)\n",
    "    print(tweet_text)\n",
    "    all_linked_entities = doc._.linkedEntities\n",
    "    for linked_entity in all_linked_entities:\n",
    "      print(linked_entity.get_url() + ' ' + linked_entity.pretty_string(True))\n",
    "  except:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Coursework_2_SMA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
