{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoEncWHm5s28"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1V7EzFr6N7p"
   },
   "source": [
    "## Library installation/import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1B0__6AYci7"
   },
   "source": [
    "Install and import libraries that are used in multiple sections of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6K2gnqqfRsoS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s7ivGrSR5NaA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (1.22.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: setuptools in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting tweepy\n",
      "  Downloading tweepy-4.6.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 4.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting requests-oauthlib<2,>=1.2.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting requests<3,>=2.27.0\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting oauthlib<4,>=3.2.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.7)\n",
      "Installing collected packages: requests, oauthlib, requests-oauthlib, tweepy\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.26.0\n",
      "    Uninstalling requests-2.26.0:\n",
      "      Successfully uninstalled requests-2.26.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.1.0 requires spacy<3.2.0,>=3.1.0, but you have spacy 3.2.1 which is incompatible.\u001b[0m\n",
      "Successfully installed oauthlib-3.2.0 requests-2.27.1 requests-oauthlib-1.3.1 tweepy-4.6.0\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.1.tar.gz (220 kB)\n",
      "\u001b[K     |████████████████████████████████| 220 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from wordcloud) (1.22.0)\n",
      "Requirement already satisfied: pillow in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from wordcloud) (9.0.0)\n",
      "Requirement already satisfied: matplotlib in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from matplotlib->wordcloud) (4.28.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/appleseo/opt/anaconda3/envs/Python3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Building wheels for collected packages: wordcloud\n",
      "  Building wheel for wordcloud (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Users/appleseo/opt/anaconda3/envs/Python3/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/kf/jxb0pf_5073ckcpwby8fzxf80000gn/T/pip-install-ma6iysp1/wordcloud_83cde0fd5eca4133af20d2b4dbea1334/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/kf/jxb0pf_5073ckcpwby8fzxf80000gn/T/pip-install-ma6iysp1/wordcloud_83cde0fd5eca4133af20d2b4dbea1334/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/kf/jxb0pf_5073ckcpwby8fzxf80000gn/T/pip-wheel-hvirjgmt\n",
      "       cwd: /private/var/folders/kf/jxb0pf_5073ckcpwby8fzxf80000gn/T/pip-install-ma6iysp1/wordcloud_83cde0fd5eca4133af20d2b4dbea1334/\n",
      "  Complete output (27 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-10.9-x86_64-3.9\n",
      "  creating build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "  copying wordcloud/wordcloud_cli.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "  copying wordcloud/_version.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "  copying wordcloud/__init__.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "  copying wordcloud/tokenization.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "  copying wordcloud/wordcloud.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "  copying wordcloud/color_from_image.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "  copying wordcloud/__main__.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "  copying wordcloud/stopwords -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "  copying wordcloud/DroidSansMono.ttf -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "  UPDATING build/lib.macosx-10.9-x86_64-3.9/wordcloud/_version.py\n",
      "  set build/lib.macosx-10.9-x86_64-3.9/wordcloud/_version.py to '1.8.1'\n",
      "  running build_ext\n",
      "  building 'wordcloud.query_integral_image' extension\n",
      "  creating build/temp.macosx-10.9-x86_64-3.9\n",
      "  creating build/temp.macosx-10.9-x86_64-3.9/wordcloud\n",
      "  x86_64-apple-darwin13.4.0-clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/appleseo/opt/anaconda3/envs/Python3/include -fPIC -O2 -isystem /Users/appleseo/opt/anaconda3/envs/Python3/include -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /Users/appleseo/opt/anaconda3/envs/Python3/include -D_FORTIFY_SOURCE=2 -isystem /Users/appleseo/opt/anaconda3/envs/Python3/include -I/Users/appleseo/opt/anaconda3/envs/Python3/include/python3.9 -c wordcloud/query_integral_image.c -o build/temp.macosx-10.9-x86_64-3.9/wordcloud/query_integral_image.o\n",
      "  x86_64-apple-darwin13.4.0-clang -bundle -undefined dynamic_lookup -Wl,-rpath,/Users/appleseo/opt/anaconda3/envs/Python3/lib -L/Users/appleseo/opt/anaconda3/envs/Python3/lib -Wl,-rpath,/Users/appleseo/opt/anaconda3/envs/Python3/lib -L/Users/appleseo/opt/anaconda3/envs/Python3/lib -Wl,-pie -Wl,-headerpad_max_install_names -Wl,-dead_strip_dylibs -Wl,-rpath,/Users/appleseo/opt/anaconda3/envs/Python3/lib -L/Users/appleseo/opt/anaconda3/envs/Python3/lib -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /Users/appleseo/opt/anaconda3/envs/Python3/include -D_FORTIFY_SOURCE=2 -isystem /Users/appleseo/opt/anaconda3/envs/Python3/include build/temp.macosx-10.9-x86_64-3.9/wordcloud/query_integral_image.o -o build/lib.macosx-10.9-x86_64-3.9/wordcloud/query_integral_image.cpython-39-darwin.so\n",
      "  ld: warning: -pie being ignored. It is only used when linking a main executable\n",
      "  ld: unsupported tapi file type '!tapi-tbd' in YAML file '/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib/libSystem.tbd' for architecture x86_64\n",
      "  clang-12: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "  error: command '/Users/appleseo/opt/anaconda3/envs/Python3/bin/x86_64-apple-darwin13.4.0-clang' failed with exit code 1\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for wordcloud\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for wordcloud\n",
      "Failed to build wordcloud\n",
      "Installing collected packages: wordcloud\n",
      "    Running setup.py install for wordcloud ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/appleseo/opt/anaconda3/envs/Python3/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/kf/jxb0pf_5073ckcpwby8fzxf80000gn/T/pip-install-ma6iysp1/wordcloud_83cde0fd5eca4133af20d2b4dbea1334/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/kf/jxb0pf_5073ckcpwby8fzxf80000gn/T/pip-install-ma6iysp1/wordcloud_83cde0fd5eca4133af20d2b4dbea1334/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/kf/jxb0pf_5073ckcpwby8fzxf80000gn/T/pip-record-xze7omkd/install-record.txt --single-version-externally-managed --compile --install-headers /Users/appleseo/opt/anaconda3/envs/Python3/include/python3.9/wordcloud\n",
      "         cwd: /private/var/folders/kf/jxb0pf_5073ckcpwby8fzxf80000gn/T/pip-install-ma6iysp1/wordcloud_83cde0fd5eca4133af20d2b4dbea1334/\n",
      "    Complete output (27 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.macosx-10.9-x86_64-3.9\n",
      "    creating build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "    copying wordcloud/wordcloud_cli.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "    copying wordcloud/_version.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "    copying wordcloud/__init__.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "    copying wordcloud/tokenization.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "    copying wordcloud/wordcloud.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "    copying wordcloud/color_from_image.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "    copying wordcloud/__main__.py -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "    copying wordcloud/stopwords -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "    copying wordcloud/DroidSansMono.ttf -> build/lib.macosx-10.9-x86_64-3.9/wordcloud\n",
      "    UPDATING build/lib.macosx-10.9-x86_64-3.9/wordcloud/_version.py\n",
      "    set build/lib.macosx-10.9-x86_64-3.9/wordcloud/_version.py to '1.8.1'\n",
      "    running build_ext\n",
      "    building 'wordcloud.query_integral_image' extension\n",
      "    creating build/temp.macosx-10.9-x86_64-3.9\n",
      "    creating build/temp.macosx-10.9-x86_64-3.9/wordcloud\n",
      "    x86_64-apple-darwin13.4.0-clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/appleseo/opt/anaconda3/envs/Python3/include -fPIC -O2 -isystem /Users/appleseo/opt/anaconda3/envs/Python3/include -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /Users/appleseo/opt/anaconda3/envs/Python3/include -D_FORTIFY_SOURCE=2 -isystem /Users/appleseo/opt/anaconda3/envs/Python3/include -I/Users/appleseo/opt/anaconda3/envs/Python3/include/python3.9 -c wordcloud/query_integral_image.c -o build/temp.macosx-10.9-x86_64-3.9/wordcloud/query_integral_image.o\n",
      "    x86_64-apple-darwin13.4.0-clang -bundle -undefined dynamic_lookup -Wl,-rpath,/Users/appleseo/opt/anaconda3/envs/Python3/lib -L/Users/appleseo/opt/anaconda3/envs/Python3/lib -Wl,-rpath,/Users/appleseo/opt/anaconda3/envs/Python3/lib -L/Users/appleseo/opt/anaconda3/envs/Python3/lib -Wl,-pie -Wl,-headerpad_max_install_names -Wl,-dead_strip_dylibs -Wl,-rpath,/Users/appleseo/opt/anaconda3/envs/Python3/lib -L/Users/appleseo/opt/anaconda3/envs/Python3/lib -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /Users/appleseo/opt/anaconda3/envs/Python3/include -D_FORTIFY_SOURCE=2 -isystem /Users/appleseo/opt/anaconda3/envs/Python3/include build/temp.macosx-10.9-x86_64-3.9/wordcloud/query_integral_image.o -o build/lib.macosx-10.9-x86_64-3.9/wordcloud/query_integral_image.cpython-39-darwin.so\n",
      "    ld: warning: -pie being ignored. It is only used when linking a main executable\n",
      "    ld: unsupported tapi file type '!tapi-tbd' in YAML file '/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib/libSystem.tbd' for architecture x86_64\n",
      "    clang-12: error: linker command failed with exit code 1 (use -v to see invocation)\n",
      "    error: command '/Users/appleseo/opt/anaconda3/envs/Python3/bin/x86_64-apple-darwin13.4.0-clang' failed with exit code 1\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /Users/appleseo/opt/anaconda3/envs/Python3/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/kf/jxb0pf_5073ckcpwby8fzxf80000gn/T/pip-install-ma6iysp1/wordcloud_83cde0fd5eca4133af20d2b4dbea1334/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/kf/jxb0pf_5073ckcpwby8fzxf80000gn/T/pip-install-ma6iysp1/wordcloud_83cde0fd5eca4133af20d2b4dbea1334/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/kf/jxb0pf_5073ckcpwby8fzxf80000gn/T/pip-record-xze7omkd/install-record.txt --single-version-externally-managed --compile --install-headers /Users/appleseo/opt/anaconda3/envs/Python3/include/python3.9/wordcloud Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!pip install tweepy\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_S1J4oc6laA"
   },
   "source": [
    "## Configuring Twitter API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_k-DQXnYXyIO"
   },
   "source": [
    "Please note that the API keys below are the course leader's own API keys. You are allowed to use it to do some small tests, but please be careful because all students in the class now have a copy of it, and hence the limits can be easily exceeded.\n",
    "\n",
    "If your group has decided to use Twitter data, you can [apply for your own keys](https://developer.twitter.com/en/apply-for-access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X0v0dZ8fCnqI"
   },
   "outputs": [],
   "source": [
    "#API Key: 4XVopc1gLxY3MaqJMMnFUmE2y\n",
    "#API Key Secret: NwcxaASpyB2vC5cwl2Xh3gkg3qpd5LfsutIJu29M8nTtkJdL1b\n",
    "#Bearer Token: AAAAAAAAAAAAAAAAAAAAAGXqaAEAAAAAnzaQhJwkS8rKcDIKkIgg3W7xr2k%3Dx23TzRFSGaH0IDkXQ33NGpu4yYwNUDg3q7uSBgg4cAB1Cp4fcJ\n",
    "\n",
    "\n",
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler('7udmMgkPoaPPiWQ6GKHONihby', 'pWZISm84iJazudbuulEvw5tn6Ayo1A0IOXQw86fhscx6aylfaJ')\n",
    "auth.set_access_token('1368127768102375428-rskpQBqbCtWCPWACaqcH6GVkxJCgd7', 'J2SbLFgsRjAp9ZiZi1dQXjM0HXpIkXvyyBmWUrobT6P4g')\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sBB2guxczux"
   },
   "source": [
    "## Downloading of new data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5EwgFk86EfS"
   },
   "source": [
    "**IMPORTANT NOTE:** Please do not run the cell below unless intending to download a new data set.\n",
    "\n",
    "Make sure that you change the parameters.\n",
    "\n",
    "Also, check the [Tweepy API reference](https://docs.tweepy.org/en/latest/api.html) to find out about other ways through which you can retrieve tweets, e.g., by specifying usernames.\n",
    "\n",
    "<font color=\"red\">**WARNING:** It is likely that the retrieved tweets, which will be displayed in succeeding cells, contain strong language that some might find offensive or disturbing.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dSQXfQVGCpmj"
   },
   "outputs": [],
   "source": [
    "#['created_at', 'id', 'id_str', 'full_text', 'truncated', 'display_text_range', 'entities', \n",
    "#'extended_entities', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', \n",
    "#'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', \n",
    "#'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', \n",
    "#'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'lang']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collect tweets\n",
    "query = \"#NetflixTop10Films\" + \" -filter:retweets\"\n",
    "cutoff_date = \"2022-01-01\"\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=query, lang=\"en\", since_id=cutoff_date).items(3000)\n",
    "\n",
    "tweets_list = [[tweet.created_at, tweet.user.screen_name, tweet.user.location, tweet.text] for tweet in tweets]\n",
    "\n",
    "tweets_df = pd.DataFrame(data=tweets_list, columns=['date', 'user', 'location', 'text'])\n",
    "\n",
    "# A good idea to save downloaded tweets as CSV\n",
    "tweets_df.to_csv ('current_set.csv', quotechar='\"', encoding='utf8', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qumfu0NwYKqR"
   },
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giyVyGPAYUTK"
   },
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K23I1qBNZ_HQ"
   },
   "source": [
    "Below we provide some code for text cleaning. However, we encourage you to think of other ways to clean your data, e.g., by removing hashtags, removing usernames, removing duplicate tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWzLqibrM0pa"
   },
   "outputs": [],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Uncomment the line below if loading from previously saved CSV\n",
    "tweets_df = pd.read_csv('current_set.csv', quotechar='\"', encoding='utf8')\n",
    "\n",
    "\n",
    "# Remove punctuation\n",
    "tweets_df['text_processed'] = tweets_df['text'].map(lambda x: re.sub('[,\\\\.!?]', ' ', x))\n",
    "\n",
    "# Remove unnecessary line breaks\n",
    "tweets_df['text_processed'] = tweets_df['text_processed'].map(lambda x: re.sub(r\"\\n\", '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "tweets_df['text_processed'] = tweets_df['text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Think of how else your data can be cleaned\n",
    "\n",
    "# Print out the first rows \n",
    "print(tweets_df['text_processed'].head())\n",
    "\n",
    "# Removing duplicate tweets?\n",
    "unique_tweets = list(set(list(tweets_df['text_processed'].values)))\n",
    "unique_tweets = [t for t in unique_tweets if t]\n",
    "\n",
    "print(unique_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22qj0jq3YklZ"
   },
   "source": [
    "## Exploration using a word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IIh30BjaWji"
   },
   "source": [
    "Generating a word cloud is one way by which you can check whether your data needs any further cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqxuMbjk8EkA"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# join the words of the different tweets together into one string\n",
    "long_string = ' '.join(unique_tweets)\n",
    "new_long_string = ' '.join(set(long_string.split(\" \")))\n",
    "\n",
    "# create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# generate a word cloud\n",
    "wordcloud.generate(new_long_string)\n",
    "\n",
    "# visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQyNo3FYYuED"
   },
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwAZb_l-7C-L"
   },
   "outputs": [],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tfx5EMNw_pIw"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Do you want to modify this by adding more stop words?\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "  return [[word for word in simple_preprocess(str(doc)) \n",
    "    if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "data = tweets_df.text_processed.values.tolist()\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "# create a dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# create a corpus\n",
    "texts = data_words\n",
    "\n",
    "# convert the corpus into a BoW representation\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOb8cNW3PVfk"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# set number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# build an LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "\n",
    "# print keywords in each topic\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RPtA2Yg7Hnm"
   },
   "outputs": [],
   "source": [
    "# visualise the topics\n",
    "!pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWueCQawPlXn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./'+str(num_topics))\n",
    "\n",
    "LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "  pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "  LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7UoGTRZY1pg"
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3RLsS3Ac8v7"
   },
   "source": [
    "This implementation is based on the lexicon- and rule-based [VADER](https://github.com/cjhutto/vaderSentiment) sentiment analysis tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gas4oUUIY8iF"
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hK8WbOHbA78"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "for tweet_text in unique_tweets:\n",
    "    vs = analyzer.polarity_scores(tweet_text)\n",
    "    print(tweet_text + '\\t' + str(vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT11HUQBY47n"
   },
   "source": [
    "# Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcanVP2udd24"
   },
   "source": [
    "This implementation is based on [spaCy's model](https://spacy.io/models/en#en_core_web_trf) using contextualised embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUWgWiLZYaBS"
   },
   "outputs": [],
   "source": [
    "!pip install spacy-transformers\n",
    "!python -m spacy download en_core_web_trf\n",
    "import spacy\n",
    "import en_core_web_trf\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAvsa90Gpwna"
   },
   "outputs": [],
   "source": [
    "for tweet_text in unique_tweets:\n",
    "  doc = nlp(tweet_text)\n",
    "  print(tweet_text)\n",
    "  for ne in doc.ents:\n",
    "    print('\\tNE found: ', ne.start_char, ne.end_char, ne.label_, tweet_text[ne.start_char:ne.end_char])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4Ub0m8zoZfH"
   },
   "source": [
    "# Named Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqLYLoM7ecLP"
   },
   "source": [
    "This implementation is based on [spaCy Entity Linker](https://github.com/egerber/spacy-entity-linker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvKRih-dYEDL"
   },
   "outputs": [],
   "source": [
    "!pip install spacy-entity-linker\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "import en_core_web_md\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBelwSqgeLjJ"
   },
   "outputs": [],
   "source": [
    "from spacy_entity_linker import EntityLinker\n",
    "!python -m spacy_entity_linker \"download_knowledge_base\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "#add to pipeline\n",
    "nlp.add_pipe('entityLinker', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnZZ0t32oCgZ"
   },
   "outputs": [],
   "source": [
    "for tweet_text in unique_tweets:\n",
    "  try:\n",
    "    doc = nlp(tweet_text)\n",
    "    print(tweet_text)\n",
    "    all_linked_entities = doc._.linkedEntities\n",
    "    for linked_entity in all_linked_entities:\n",
    "      print(linked_entity.get_url() + ' ' + linked_entity.pretty_string(True))\n",
    "  except:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Coursework_2_SMA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
